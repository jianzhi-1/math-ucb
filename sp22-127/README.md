# EECS 127
### Optimization Models in Engineering
UC Berkeley Spring 2022, taught by Prof Thomas Courtade

- [x] Week 1: Introduction (lec1); Linear algebra review (lec2)
- [ ] Week 2:
- [ ] Week 3:


â„‚, â„, Î©, âˆ, âˆ€, â‰¥, â‰¤, âˆˆ, âˆ‰, âŠ†, âŠ‚, Ã˜, â†’, Ã—, â€–, Î£, Â·, âˆ€, âˆ‡, â‡’, âŸ¨, âŸ©, âˆ‚

### Multivariable Calculus
- Gradient of *f:â„<sup>n</sup> â†’ â„* at *x<sub>0</sub>* is a *n*-vector given by: *âˆ‡f = \[âˆ‚f/âˆ‚x<sub>i</sub>]
- *g(x) = f(Ax + b) â‡’ âˆ‡g = A<sup>T</sup>âˆ‡f(Ax + b)* 
- Gradient is perpendicular to level sets, and points in the direction of steepest increase.
- First order approximation of *f(x)*: *f(x<sub>0</sub>) + âˆ‡f(x<sub>0</sub>)<sup>T</sup>(x - x<sub>0</sub>)*
- First order approximation of *f(x)*: *f(x) = f(x<sub>0</sub>) + J(x - x<sub>0</sub>)* where *J* is the **Jacobian matrix** of *f* at *x<sub>0</sub>*. (so *J<sub>ij</sub> = âˆ‚f<sub>i</sub>/âˆ‚x<sub>j</sub>*.

### Linear Algebra

##### First Principles
- Hyperplane *H = {x | a<sup>T</sup>x = b}*
- Equivalently, if *x<sub>0</sub> âˆˆ H*, then a hyperplane is the set of vectors *x* s.t. *x - x<sub>0</sub>* is orthogonal to *a*.
- Projection of *0* to the hyperplane: point *ba* (assuming *a* normalized)
- Geometrically, *|b|* is the length of closest point *x* on the hyperplane from the origin.
- Halfspace *H = {x | a<sup>T</sup>x â‰¥ b}*
- For any two matrices *A âˆˆ â„<sup>m Ã— n</sup>, B âˆˆ â„<sup>n Ã— m</sup>*, *tr(AB) = tr(BA)*
- The scalar product of two same type matrices is symmetric and is the sum of product of respective components: *âŸ¨A, BâŸ© = tr(A<sup>T</sup>B)*

##### Properties of Special Matrices

###### Full Column Rank Matrices
- A matrix *A âˆˆ â„<sup>m Ã— n</sup>* has full column rank if all columns are linearly independent. (implies *m â‰¥ n*)
- A matrix *A* has full column rank if and only if *âˆƒB âˆˆ â„<sup>n Ã— m</sup>* s.t. *BA = I<sub>n</sub>*
- *A = Q\[R<sub>1</sub> 0]<sup>T</sup>*
- *B = \[R<sub>1</sub><sup>-1</sup> 0]Q<sup>T</sup>*
- *B = (A<sup>T</sup>A)<sup>-1</sup>A<sup>T</sup>*
- In general, left inverses are not unique.


###### Full Row Rank Matrices
- A matrix *A âˆˆ â„<sup>m Ã— n</sup>* has full row rank if all rows are linearly independent. (implies *n â‰¥ m*)
- A matrix *A* has full row rank if and only if *âˆƒB âˆˆ â„<sup>n Ã— m</sup>* s.t. *AB = I<sub>m</sub>*
- *A<sup>T</sup> = Q\[R<sub>1</sub><sup>T</sup> 0]<sup>T</sup>*
- *A = \[R<sub>1</sub><sup>T</sup> 0]Q<sup>T</sup>*
- *B = A<sup>T</sup>(AA<sup>T</sup>)<sup>-1</sup>*
- In general, right inverses are not unique.


###### Symmetric Matrices

###### Orthogonal Matrices
- *U<sup>T</sup>U = UU<sup>T</sup> = I<sub>n</sub>*
- Preserves length and angles: *âŸ¨Ux, UyâŸ© = âŸ¨x, yâŸ©*

###### Dyads
- *A = uv<sup>T</sup>* for some *u, v*
- *Ax = uv<sup>T</sup>x = (v<sup>T</sup>x)u âˆˆ Span(u) âˆ€x*

â„‚, â„, Î©, âˆ, âˆ€, â‰¥, â‰¤, âˆˆ, âˆ‰, âŠ†, âŠ‚, Ã˜, â†’, Ã—, â€–, Î£, Â·, âˆ€, âˆ‡, â‡’, âŸ¨, âŸ©

##### QR Decomposition
- Factorizes a matrix *A = QR* where *Q* is orthogonal (i.e. *Q<sup>T</sup>Q = I*) and *R* is upper-triangular.
- *A âˆˆ â„<sup>m Ã— n</sup>* is full rank (*rank(A) = n =* number of columns of *A*)
  - *Q = \[q<sub>1</sub> ... q<sub>n</sub>]* where *q<sub>i</sub>* are the vectors from Gram-Schmidt process on *a<sub>i</sub>*
  - *R âˆˆ â„<sup>n Ã— n</sup>* is equal to the coefficients obtained during Gram-Schmidt (either a projection or a scaling factor)
  - *R* is invertible implies *A* is full rank.
- *A âˆˆ â„<sup>m Ã— n</sup>* is **not** full rank
  - Key idea is to permutate the matrix *A* such that all the linearly independent column vectors come first.
  - *AP = Q\[R<sub>1</sub> R<sub>2</sub>]*
  - *Q âˆˆ â„<sup>m Ã— r</sup> = \[q<sub>1</sub> ... q<sub>r</sub>]* where *q<sub>i</sub>* are the vectors from Gram-Schmidt process
  - *R<sub>1</sub> âˆˆ â„<sup>r Ã— r</sup>* is a square, invertible, upper-triangular matrix and equal to the coefficients obtained during Gram-Schmidt
  - *R<sub>2</sub> âˆˆ â„<sup>r Ã— (n-r)</sup>* is the equal to the coefficients obtained during Gram-Schmidt of the dependent column vectors of *A*
  - *P* is a permutation (therefore orthogonal) matrix that moves all independent column vectors to the left
- Full QR Decomposition
  - Key idea: append *I<sub>m</sub>* to the matrix *A*
  - *AP = QR*
  - *Q âˆˆ â„<sup>m Ã— m</sup>* is always orthogonal (since we can always find *m* independent vectors)
  - *R âˆˆ â„<sup>m Ã— (n+m)</sup> = \[\[R<sub>1</sub> R<sub>2</sub>] \[0 0]]* where *R<sub>1</sub>* is invertible and gives *rank(A)*.
  - *P* is a permutation (therefore orthogonal) matrix

##### Norms
###### *l<sub>p</sub>* Norm
- *â€–xâ€–<sub>p</sub> = (Î£<sub>i</sub>x<sub>i</sub><sup>p</sup>)<sup>1/p</sup>*, *p âˆˆ â„*
- *â€–xâ€–<sub>âˆ</sub> = max<sub>i</sub>|x<sub>i</sub>|*

###### Dual Norm
- For a given norm *â€–Â·â€–*, define the **dual norm** *â€–Â·â€–<sub>\*</sub>* s.t. *â€–yâ€–<sub>\*</sub> = max<sub>x</sub> x<sup>T</sup>y* with *â€–xâ€– â‰¤ 1*.
  - Intuitively, the dual norm of a vector is the maximum value achieved after applying a linear function with norm *1* to it.
  - On Wikipedia, dual norm is the measure of size for a continuous linear function defined on vector space.
- *x<sup>T</sup>y â‰¤ â€–xâ€–â€–yâ€–<sub>\*</sub>*
- Norm dual to the Euclidean norm is itself.
- Norm dual to the *l<sub>âˆ</sub>* norm is the *l<sub>1</sub>* norm

###### Operator Norm
- Given two normed vector spaces *V, W*, a linear map *A:V â†’ W* is **continuous** iff *âˆƒc* s.t. *â€–Avâ€– â‰¤ câ€–vâ€– âˆ€v âˆˆ V*
- Intuitively, a continuous operator *A* never increases a vector by more than a factor of *c*.
- Consequence: image of a bounded set is bounded, so all continuous operators are bounded operators.
- *â€–Aâ€–<sub>op</sub> := inf\{c â‰¥ 0 | â€–Avâ€– â‰¤ câ€–vâ€– âˆ€v âˆˆ V}*
- TODO: there are multiple other equivalent definitions

###### Frobenius Norm
- *â€–Aâ€–<sub>F</sub> = sqrt(Tr(A<sup>T</sup>A))*
- Equivalent to the Euclidean norm of treating *A* as a vector of length *nm*
- Invariant under orthonormal transformation of basis
- Intuitively, captures the *average effect of noise*.

###### Largest Singular Value (LSV) Norm
- *â€–Aâ€–<sub>LSV</sub> = max<sub>v | â€–vâ€–<sub>2</sub> &lt; 1</sub> â€–Avâ€–<sub>2</sub>*
- Intuitively, captures the *worst case effect of noise*.

â„‚, â„, Î©, âˆ, âˆ€, â‰¥, â‰¤, âˆˆ, âˆ‰, âŠ†, âŠ‚, Ã˜, â†’, Ã—, â€–, Î£, Â·, âˆ€, âˆ‡, â‡’, âŸ¨, âŸ©
###### Variant of LSV Norm
- *â€–Aâ€–<sub>âˆ, 1</sub> = max<sub>v | â€–vâ€–<sub>âˆ</sub> &lt; 1</sub> â€–Avâ€–<sub>1</sub>*
- Intuitively, captures the *worst case effect of noise*, but considers all vectors *v* with largest component less than *1*.

##### Projection
- Line *x<sub>0</sub> + tu*, *u* not necessarily normalized; point *x*.
- Optimal *t*: *t<sup>\*</sup> = (u<sup>T</sup>(x - x<sub>0</sub>)/(u<sup>T</sup>u)*
- Projected vector: *z<sup>\*</sup> = x<sub>0</sub> + (u<sup>T</sup>(x - x<sub>0</sub>))/(u<sup>T</sup>u) u*

##### Singular Value Decomposition

##### Principal Component Analysis

##### Linear Programming
- Farkas' Lemma: Let *A âˆˆ â„<sup>mÃ—n</sup>* and *b âˆˆ â„<sup>n</sup>*, then **exactly one** of the following assertions is true.
  - There exists *x âˆˆ â„<sup>n</sup>* such that *Ax = b* and *x â‰¥ 0*.
  - There exists *y âˆˆ â„<sup>m</sup>* such that *A<sup>T</sup>y â‰¥ 0* and *b<sup>T</sup>y < 0*.

### Duality

###### Conjugate (Fenchel) Duality

###### Weak Duality

###### Strong Duality Theorem

##### Convex Duality

### Optimization

##### Conic Optimization

##### Convex Optimization



### Applications

### Bag of Tricks
- Inequalities (AM-GM, CS, Holder, Muirhead, Jensen, Power)
- Geometric interpretation
- Smoothing

### Matlab
```Matlab
>> x = [1; 2; -3];
>> r2 = norm(x,2); % l2-norm
>> r1 = norm(x,1); % l1 norm
>> rinf = norm(x,inf); % l-infty norm
```

```Matlab
>> [Q,R] = qr(A,0); % A is a mxn matrix, Q is mxn orthogonal, R is nxn upper triangular
```

```Matlab
>> Ainv = inv(A); % produces the inverse of a square, invertible matrix
```

```Matlab
>> frob_norm = norm(A,'fro');
>> lsv_norm = norm(A);
```


### Exam Area

#### Midterm Prep ğŸ˜¤
- [ ] YY Spring/Fall

#### Final Exam Prep ğŸ˜¤
- [ ] YY Spring/Fall
